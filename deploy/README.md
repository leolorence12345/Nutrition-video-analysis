# Nutrition Video Analysis API - Deployment Guide

Production-ready REST API for automated food recognition and calorie estimation from videos.

## ğŸ—ï¸ Architecture

```
User â†’ FastAPI â†’ [Florence-2] â†’ [SAM2] â†’ [Metric3D] â†’ [RAG] â†’ Results
                      â†“              â†“          â†“           â†“
                  Detection    Tracking    Depth    Nutrition DB
```

## ğŸ“‹ Prerequisites

- **GPU**: NVIDIA GPU with CUDA 11.8+ (8GB+ VRAM recommended)
- **Docker**: Docker 20.10+ with nvidia-docker2
- **RAM**: 16GB+ recommended
- **Storage**: 50GB+ for models and data

## ğŸš€ Quick Start

### 1. Prepare Environment

```bash
# Navigate to deploy directory
cd deploy

# Copy environment template
cp env.example .env

# Edit .env with your configuration
nano .env  # Add your GEMINI_API_KEY
```

### 2. Build and Run

```bash
# Build Docker image
docker-compose build

# Start API service
docker-compose up -d

# Check logs
docker-compose logs -f nutrition-api
```

### 3. Test API

```bash
# Health check
curl http://localhost:8000/health

# Upload video
curl -X POST "http://localhost:8000/api/upload" \
  -F "file=@/path/to/video.mp4"

# Get status (use job_id from upload response)
curl http://localhost:8000/api/status/{job_id}

# Get results
curl http://localhost:8000/api/results/{job_id}
```

## ğŸ“ Project Structure

```
deploy/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ config.py          # Configuration management
â”‚   â”œâ”€â”€ models.py          # Model loading & caching
â”‚   â”œâ”€â”€ pipeline.py        # Main processing pipeline
â”‚   â”œâ”€â”€ api.py             # FastAPI endpoints
â”‚   â””â”€â”€ database.py        # Job tracking database
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ uploads/           # Uploaded videos
â”‚   â”œâ”€â”€ outputs/           # Processing results
â”‚   â””â”€â”€ rag/              # Nutrition databases
â”œâ”€â”€ Dockerfile            # Container definition
â”œâ”€â”€ docker-compose.yml    # Service orchestration
â”œâ”€â”€ requirements.txt      # Python dependencies
â””â”€â”€ README.md            # This file
```

## ğŸ”§ Configuration

All settings are controlled via environment variables in `.env`:

### Essential Settings

```env
# Required: Gemini API key for calorie fallback
GEMINI_API_KEY=your-key-here

# GPU device (cuda or cpu)
DEVICE=cuda

# Video processing
MAX_VIDEO_SIZE_MB=500
FRAME_SKIP=10
MAX_FRAMES=60
```

### Advanced Settings

See `env.example` for all available options including:
- Database configuration (SQLite/PostgreSQL)
- Model paths and variants
- Tracking parameters
- Security settings

## ğŸ¯ API Endpoints

### POST `/api/upload`
Upload a video for analysis
```bash
curl -X POST -F "file=@video.mp4" http://localhost:8000/api/upload
```

Response:
```json
{
  "job_id": "abc-123",
  "status": "processing",
  "message": "Video uploaded successfully"
}
```

### GET `/api/status/{job_id}`
Check processing status
```bash
curl http://localhost:8000/api/status/abc-123
```

### GET `/api/results/{job_id}`
Get nutrition analysis results
```bash
curl http://localhost:8000/api/results/abc-123?detailed=true
```

Response:
```json
{
  "job_id": "abc-123",
  "status": "completed",
  "nutrition_summary": {
    "total_calories_kcal": 650,
    "total_mass_g": 425,
    "num_food_items": 3
  },
  "detailed_results": {...}
}
```

### GET `/api/download/{job_id}`
Download full results as JSON

### DELETE `/api/jobs/{job_id}`
Delete job and associated files

## ğŸ³ Docker Commands

```bash
# Build image
docker-compose build

# Start services
docker-compose up -d

# View logs
docker-compose logs -f

# Stop services
docker-compose down

# Restart
docker-compose restart

# Access container shell
docker-compose exec nutrition-api bash

# Clean up everything
docker-compose down -v
```

## â˜ï¸ Cloud Deployment

### AWS EC2 with GPU

1. **Launch Instance**
   ```bash
   # Instance type: g4dn.xlarge (T4 GPU, $0.52/hr)
   # AMI: Deep Learning AMI (Ubuntu 22.04)
   # Storage: 100GB+ EBS
   ```

2. **Install Docker & nvidia-docker**
   ```bash
   # Install Docker
   curl -fsSL https://get.docker.com -o get-docker.sh
   sudo sh get-docker.sh
   
   # Install nvidia-docker
   distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
   curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
   curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \
     sudo tee /etc/apt/sources.list.d/nvidia-docker.list
   
   sudo apt-get update
   sudo apt-get install -y nvidia-docker2
   sudo systemctl restart docker
   ```

3. **Deploy Application**
   ```bash
   git clone <your-repo>
   cd deploy
   cp env.example .env
   # Edit .env with production values
   docker-compose up -d
   ```

4. **Setup NGINX (optional)**
   ```bash
   sudo apt-get install nginx
   # Configure reverse proxy
   ```

### Google Cloud Platform

```bash
# Create instance with GPU
gcloud compute instances create nutrition-api \
  --zone=us-central1-a \
  --machine-type=n1-standard-4 \
  --accelerator=type=nvidia-tesla-t4,count=1 \
  --image-family=pytorch-latest-gpu \
  --image-project=deeplearning-platform-release \
  --boot-disk-size=100GB

# SSH and deploy
gcloud compute ssh nutrition-api
```

### Kubernetes

```yaml
# deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nutrition-api
spec:
  replicas: 2
  template:
    spec:
      containers:
      - name: api
        image: your-registry/nutrition-api:latest
        resources:
          limits:
            nvidia.com/gpu: 1
        ports:
        - containerPort: 8000
```

## ğŸ“Š Monitoring

### Health Checks

```bash
# API health
curl http://localhost:8000/health

# GPU status
docker-compose exec nutrition-api nvidia-smi
```

### Logs

```bash
# Real-time logs
docker-compose logs -f

# Last 100 lines
docker-compose logs --tail=100

# Specific service
docker-compose logs nutrition-api
```

### Metrics (Optional)

Add Prometheus monitoring:
```yaml
# In docker-compose.yml
services:
  prometheus:
    image: prom/prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
```

## ğŸ”’ Security Checklist

- [ ] Set strong `API_KEY` in production
- [ ] Enable `REQUIRE_API_KEY=True`
- [ ] Restrict `CORS_ORIGINS` to your domain
- [ ] Use HTTPS (setup SSL certificate)
- [ ] Enable firewall rules (only port 443/80)
- [ ] Rotate Gemini API key regularly
- [ ] Setup automated backups for database
- [ ] Use secrets manager for sensitive data

## ğŸ› Troubleshooting

### GPU Not Detected
```bash
# Test GPU in container
docker run --rm --gpus all nvidia/cuda:11.8.0-base nvidia-smi

# Check nvidia-docker
sudo systemctl status nvidia-docker
```

### Out of Memory
- Reduce `BATCH_SIZE` in .env
- Reduce `MAX_FRAMES` to process fewer frames
- Increase `FRAME_SKIP` to skip more frames
- Use smaller model: `SAM2_CHECKPOINT=sam2.1_hiera_small.pt`

### Slow Processing
- Check GPU utilization: `nvidia-smi`
- Enable FP16: `USE_FP16=True`
- Reduce video resolution: `RESIZE_WIDTH=640`

### Model Download Failures
```bash
# Manually download models
docker-compose exec nutrition-api python3 -c "
from transformers import AutoModel
AutoModel.from_pretrained('microsoft/Florence-2-base-ft')
"
```

## ğŸ“ˆ Performance Optimization

### For Speed
- Use FP16: `USE_FP16=True`
- Skip frames: `FRAME_SKIP=15`
- Reduce resolution: `RESIZE_WIDTH=640`
- Use smaller models: `metric3d_vit_small`

### For Accuracy
- Process more frames: `FRAME_SKIP=5`
- Higher resolution: `RESIZE_WIDTH=1024`
- Larger models: `SAM2_CHECKPOINT=sam2.1_hiera_large.pt`

## ğŸ’° Cost Estimation

### AWS g4dn.xlarge (T4 GPU)
- **Instance**: $0.52/hour
- **Storage**: $0.10/GB/month
- **Transfer**: $0.09/GB

**Monthly estimate (1000 videos/day):**
- Compute (8hrs/day): ~$125
- Storage (100GB): $10
- **Total**: ~$135/month

### Auto-scaling
```bash
# Scale down when queue is empty
docker-compose scale nutrition-api=0

# Scale up during peak hours
docker-compose scale nutrition-api=3
```

## ğŸ“ License & Attribution

This deployment uses:
- **SAM2**: Meta's Segment Anything Model 2
- **Florence-2**: Microsoft's vision foundation model
- **Metric3D**: Depth estimation model
- **FastAPI**: Modern web framework

## ğŸ¤ Support

For issues and questions:
1. Check logs: `docker-compose logs`
2. Review troubleshooting section above
3. Open GitHub issue with logs and configuration

## ğŸ¯ Next Steps

1. âœ… Deploy to cloud (AWS/GCP)
2. âœ… Setup monitoring (Prometheus/Grafana)
3. âœ… Implement caching (Redis)
4. âœ… Add authentication (JWT)
5. âœ… Create web dashboard
6. âœ… Mobile app integration
7. âœ… A/B testing framework
8. âœ… Export to PDF reports

